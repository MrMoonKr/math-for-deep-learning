# 차례


## **1. 실습 환경 구축 *( Setting the Stage )***  

  - 1.1 도구 모음 설치 *( Installing the Toolkits )*  
    - 1.1.1 리눅스 *( Linux )*  
    - 1.1.2 맥OS *( macOS )*  
    - 1.1.3 윈도우 *( Windows )*  

  - 1.2 넘파이 *( NumPy )*  
    - 1.2.1 배열 정의 *( Defining Arrays )*
    - 1.2.2 데이터 형식 *( Data Types )*
    - 1.2.3 2차원 배열 *( 2D Arrays )*
    - 1.2.4 영 배열과 원 배열 *( Zeros and Ones )*
    - 1.2.5 고급 색인 조회 *( Advanced Indexing )*
    - 1.2.6 디스크 읽기와 쓰기 *( Reading and Writing to Disk )*

  - 1.3 사이파이 *( SciPy )*

  - 1.4 맷플롯립 *( Matplotlib )*

  - 1.5 사이킷런 *( Scikit-Learn )*

  - 1.6 요약 *( Summary )*

---


## **2. 확률 1부 *( Probability )***  

  - 2.1 기본 개념 *( Basic Concepts )*  
    - 2.1.1 표본공간과 사건 *( Sample Space and Events )*  
    - 2.1.2 확률변수 *( Random Variables )*  
    - 2.1.3 인간은 확률에 약하다 *( Humans Are Bad at Probability )*  

  - 2.2 확률의 법칙들 *( The Rules of Probability )*  
    - 2.2.1 단일 사건의 확률 *( Probability of an Event )*  
    - 2.2.2 합 법칙 *( Sum Rule )*  
    - 2.2.3 곱 법칙 *( Product Rule )*  
    - 2.2.4 합법칙 재논의 *( Sum Rule Revisited )*  
    - 2.2.5 생일 역설 *( The Birthday Paradox )*  
    - 2.2.6 조건부 확률 *( Conditional Probability )*  
    - 2.2.7 전체확률 *( Total Probability )*  

  - 2.3 결합 확률과 주변 확률 *( Joint and Marginal Probability )*  
    - 2.3.1 결합 확률표 *( Joint Probability Tables )*  
    - 2.3.2 확률의 연쇄법칙 *( Chain Rule for Probability )*  

  - 2.4 요약 *( Summary )*

---


## **3. 확률 2부 *( More Probability )***  

  - 3.1 확률 분포 *( Probability Distributions )*  
    - 3.1.1 히스토그램과 확률 *( Histograms and Probabilities )*  
    - 3.1.2 이산 확률분포 *( Discrete Probability Distributions )*  
    - 3.1.3 연속 확률분포 *( Continuous Probability Distributions )*  
    - 3.1.4 중심 극한 정리 *( Central Limit Theorem )*  
    - 3.1.5 대수의 법칙 *( The Law of Large Numbers )*  
      
  - 3.2 베이즈 정리 *( Bayes’ Theorem )*  
    - 3.2.1 암인가 아닌가 재논의 *( Cancer or Not Redux )*  
    - 3.2.2 사전 확률 갱신 *( Updating the Prior )*  
    - 3.2.3 머신러닝에서의 베이즈 정리 *( Bayes’ Theorem in Machine Learning )*  

  - 3.5 요약 *( Summary )*  

---


## **4장. 통계 *( Statistics )***  

  - 4.1 데이터의 종류 *( Types of Data )*  
    - 4.1.1 명목형 데이터 *( Nominal Data )*  
    - 4.1.2 순서형 데이터 (Ordinal Data )*  
    - 4.1.3 구간형 데이터 (Interval Data )*  
    - 4.1.4 비율형 데이터 (Ratio Data )*  
    - 4.1.5 딥러닝에서 명목형 데이터 사용하기 *( Using Nominal Data in Deep Learning )*  

  - 4.2 요약 통계 *( Summary Statistics )*  
    - 4.2.1 평균과 중앙값 *( Means and Median )*  
    - 4.2.2 산포도 측정값 *( Measures of Variation )*  

  - 4.3 분위수와 상자그림 *( Quantiles and Box Plots )*  

  - 4.4 결측 데이터 *( Missing Data )*  

  - 4.5 상관관계 *( Correlation )*  
    - 4.5.1 피어슨 상관계수 *( Pearson Correlation )*  
    - 4.5.2 스피어만 상관계수 *( Spearman Correlation )*  

  - 4.6 가설검정 *( Hypothesis Testing )*  
    - 4.6.1 가설의 구조 *( Hypotheses )*  
    - 4.6.2 t-검정 *( The t-test )*  
    - 4.6.3 맨-휘트니 U 검정 *( The Mann–Whitney U Test )*  

  - 4.7 요약 *( Summary )*

---


## **5장. 선형대수 1부 *( Linear Algebra )***

  - 5.1 스칼라, 벡터, 행렬, 텐서 *( Scalars, Vectors, Matrices, and Tensors )*  

  - 5.2 텐서의 산술 연산 *( Arithmetic with Tensors )*  
    - 5.2.1 배열 연산 *( Array Operations )*  
    - 5.2.2 벡터 연산 *( Vector Operations )*  
    - 5.2.3 행렬곱 *( Matrix Multiplication )*  
    - 5.2.4 크로네커 곱 *( Kronecker Product )*  

  - 5.3 요약 *( Summary )*  

---


## **6장. 선형대수 2부 *( More Linear Algebra )***  

  - 6.1 정방행렬 *( Square Matrices )*  
    - 6.1.1 왜 정방행렬인가? *( Why Square Matrices? )*  
    - 6.1.2 전치, 대각합, 거듭제곱 *( Transpose, Trace, and Powers )*  
    - 6.1.3 특별한 정방행렬 *( Special Square Matrices )*  
    - 6.1.4 단위행렬 *( The Identity Matrix )*  
    - 6.1.5 행렬식 *( Determinants )*  
    - 6.1.6 역행렬 *( Inverses )*  
    - 6.1.6 대칭행렬, 직교행렬, 유니터리 행렬 *( Symmetric, Orthogonal, and Unitary Matrices )*  
    - 6.1.7 대칭행렬의 정부호성 *( Definiteness of a Symmetric Matrix )*  
      
  - 6.2 고유값과 고유벡터 *( Eigenvalues and Eigenvectors )*  
    - 6.2.1 고유값/고유벡터 구하기 *( Finding Eigenvalues and Eigenvectors )*  

  - 6.3 벡터 노름과 거리 함수 *( Vector Norms and Distance Metrics )*  
    - 6.3.1 L-노름과 거리 함수 *( L-Norms and Distance Metrics )*  
    - 6.3.2 공분산 행렬 *( Covariance Matrices )*  
    - 6.3.3 마할라노비스 거리 *( Mahalanobis Distance )*
    - 6.3.4 쿨백–라이블러 발산 *( Kullback–Leibler Divergence )*

  - 6.4 주성분 분석 *( Principal Component Analysis )*  

  - 6.5 특이값 분해 및 유사 역행렬 *( Singular Value Decomposition and Pseudoinverse )*  
    - 6.5.1 특이값 분해의 실제 활용 *( SVD in Action )*  
    - 6.5.2 두 가지 응용 *( Two Applications )*  

  - 6.6 요약 *( Summary )*  

---


## **7장. 미분 *( Differential Calculus )***

  - 7.1 기울기 *( Slope )*  

  - 7.2 도함수 *( Derivatives )*  
    - 7.2.1 도함수의 공식적인 정의 *( A Formal Definition )*  
    - 7.2.2 기본 미분 법칙 *( Basic Rules )*  
    - 7.2.3 삼각함수 미분 법칙 *( Rules for Trigonometric Functions )*  
    - 7.2.4 지수·로그 미분 법칙 *( Rules for Exponentials and Logarithms )*  

  - 7.3 함수의 극소값과 극대값 *( Minima and Maxima of Functions )*  

  - 7.4 편미분 *( Partial Derivatives )*  
    - 7.4.1 혼합 편미분 *( Mixed Partial Derivatives )*  
    - 7.4.2 편미분 연쇄법칙 *( Chain Rule for Partial Derivatives )*  

  - 7.5 경사 *( Gradients )*  
    - 7.5.1 경사 계산하기 *( Calculating the Gradient )*  
    - 7.5.2 경사 시각화 *( Visualizing the Gradient )*  

  - 7.6 요약 *( Summary )*  

---


## **8장. 행렬 미분 *( Matrix Calculus )***  

  - 8.1 공식들 *( The Formulas )*  
    - 8.1.1 스칼라 → 벡터 함수 *( A Vector Function by a Scalar Argument )*  
    - 8.1.2 벡터 → 스칼라 함수 *( A Scalar Function by a Vector Argument )*  
    - 8.1.3 벡터 → 벡터 함수 *( A Vector Function by a Vector )*  
    - 8.1.4 스칼라 → 행렬 함수 *( A Matrix Function by a Scalar )*  
    - 8.1.5 행렬 → 스칼라 함수 *( A Scalar Function by a Matrix )*  

  - 8.2 항등식 *( The Identities )*  
    - 8.2.1 벡터 → 스칼라 *( A Scalar Function by a Vector )*  
    - 8.2.2 스칼라 → 벡터 *( A Vector Function by a Scalar )*  
    - 8.2.3 벡터 → 벡터 *( A Vector Function by a Vector )*  
    - 8.2.4 행렬 → 스칼라 *( A Scalar Function by a Matrix )*  

  - 8.3 야코비안 행렬과 헤시안 행렬 *( Jacobians and Hessians )*  
    - 8.3.1 야코비안 행렬 *( Jacobians )*
    - 8.3.2 헤시안 행렬 *( Hessians )*
    
  - 8.4 행렬 미분 예제들 *( Some Examples of Matrix Calculus Derivatives )*  
    - 8.4.1 성분별 연산의 미분 *( Derivative of Element-Wise Operations )*  
    - 8.4.2 활성화 함수의 미분 *( Derivative of the Activation Function )*  

  - 8.5 요약 *( Summary )*  

---


## **9장. 신경망의 데이터 흐름 *( Data Flow in Neural Networks )***

  - 9.1 데이터 표현 *( Representing Data )*  
    - 9.1.1 전통적 신경망 *( Traditional Neural Networks )*  
    - 9.1.2 심층 합성곱 신경망 *( Deep Convolutional Networks )*  

  - 9.2 전통적인 신경망의 데이터 흐름 *( Data Flow in Traditional Neural Networks )*  

  - 9.3 합성곱 신경망의 데이터 흐름 *( Data Flow in Convolutional Neural Networks )*  
    - 9.3.1 합성곱 *( Convolution )*  
    - 9.3.2 합성곱 층 *( Convolutional Layers )*  
    - 9.3.3 풀링 층 *( Pooling Layers )*  
    - 9.3.4 완전연결층 *( Fully Connected Layers )*  
    - 9.3.5 합성곱 신경망의 데이터 흐름 *( Data Flow in Convolutional Neural Networks )*  

  - 9.4 요약 *( Summary )*  

---


## **10장. 역전파 *( Backpropagation )***

  - 10.1 역전파란 무엇인가? *( What Is Backpropagation? )*  

  - 10.2 직접 계산해 보는 역전파 *( Backpropagation by Hand )*  
    - 10.2.1 편미분 유도 *( Calculating the Partial Derivatives )*  
    - 10.2.2 파이썬 구현 *( Translating into Python )*  
    - 10.2.3 신경망 모델 학습과 검증 *( Training and Testing the Model )*  

  - 10.3 완전 연결 신경망의 역전파 *( Backpropagation for Fully Connected Networks )*  
    - 10.3.1 오차 역전파 *( Backpropagating the Error )*  
    - 10.3.2 가중치 및 치우침 계산 *( Calculating Partial Derivatives of the Weights and Biases )*  
    - 10.3.3 파이썬 구현 *( A Python Implementation )*  
    - 10.3.4 구현 적용 *( Using the Implementation )*  

  - 10.4 계산 그래프 *( Computational Graphs )*  

  - 10.5 요약 *( Summary )*  

---


## **11장. 경사하강법 *( Gradient Descent )***

  - 11.1 기본 개념 *( The Basic Idea )*  
    - 11.1.1 1차원 경사하강법 *( Gradient Descent in One Dimension )*  
    - 11.1.2 2차원 경사하강법 *( Gradient Descent in Two Dimensions )*  

  - 11.2 확률적 경사하강법 *( Stochastic Gradient Descent )*  

  - 11.3 운동량 모멘텀 *( Momentum )*  
    - 11.3.1 1D/2D 모멘텀 (Momentum in 1D, Momentum in 2D)
    - 11.3.2 모멘텀을 사용한 학습 (Training Models with Momentum)
    - 11.3.3 네스테로프 모멘텀 (Nesterov Momentum)

  - 11.4 적응형 경사하강법 *( Adaptive Gradient Descent )*  
    - 11.4.1 RMSprop
    - 11.4.2 Adagrad & Adadelta
    - 11.4.3 Adam
    - 11.4.4 옵티마이저에 대한 생각들 (Some Thoughts About Optimizers)

  - 11.5 요약 *( Summary )*  

---


## **부록 *( Appendix : Going Further )***

  - 확률과 통계 *( Probability and Statistics )*  
  - 선형대수 *( Linear Algebra )*
  - 미적분 *( Calculus )*
  - 심층학습 *( Deep Learning )*  

---

